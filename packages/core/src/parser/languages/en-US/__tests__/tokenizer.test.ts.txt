// packages/core/src/parser/languages/en-US/__tests__/tokenizer.test.ts

import { createEnglishTokenizer } from '../tokenizer';
import { TokenType } from '../../../core/types';

describe('EnglishTokenizer', () => {
  let tokenizer: ReturnType<typeof createEnglishTokenizer>;

  beforeEach(() => {
    tokenizer = createEnglishTokenizer();
  });

  describe('basic tokenization', () => {
    it('should tokenize a simple command', () => {
      const input = 'take the red key';
      const tokens = tokenizer.tokenize(input);
      
      expect(tokens.length).toBe(4);
      expect(tokens[0].text).toBe('take');
      expect(tokens[0].type).toBe(TokenType.WORD);
      expect(tokens[0].normalized).toBe('take');
      
      expect(tokens[1].text).toBe('the');
      expect(tokens[1].type).toBe(TokenType.WORD);
      
      expect(tokens[3].text).toBe('key');
      expect(tokens[3].type).toBe(TokenType.WORD);
    });
  });

  describe('contraction handling', () => {
    it('should recognize contractions as single tokens', () => {
      const input = "don't take the key";
      const tokens = tokenizer.tokenize(input);
      
      expect(tokens.length).toBe(4);
      expect(tokens[0].text).toBe("don't");
      expect(tokens[0].type).toBe(TokenType.WORD);
      expect(tokens[0].normalized).toBe("don't");
      
      // Check for expansion metadata
      expect(tokens[0]).toHaveProperty('expansion');
      expect(tokens[0].expansion).toBe('do not');
    });

    it('should handle disabled contraction expansion', () => {
      const tokenizerNoContractions = createEnglishTokenizer({
        handleContractions: false
      });
      
      const input = "don't take the key";
      const tokens = tokenizerNoContractions.tokenize(input);
      
      // Still recognizes "don't" as a single token, but without expansion
      expect(tokens[0].text).toBe("don't");
      expect(tokens[0]).not.toHaveProperty('expansion');
    });

    it('should handle various English contractions', () => {
      const contractions = [
        "I'll", "you're", "he's", "she's", "we're", 
        "they're", "isn't", "aren't", "wasn't", "weren't",
        "hasn't", "haven't", "couldn't", "wouldn't", "shouldn't"
      ];
      
      for (const contraction of contractions) {
        const input = `${contraction} be recognized`;
        const tokens = tokenizer.tokenize(input);
        
        expect(tokens[0].text).toBe(contraction);
        expect(tokens[0].type).toBe(TokenType.WORD);
        expect(tokens[0]).toHaveProperty('expansion');
      }
    });
  });

  describe('compound verb handling', () => {
    it('should recognize compound verbs as single tokens', () => {
      const input = 'pick up the red book';
      const tokens = tokenizer.tokenize(input);
      
      expect(tokens.length).toBe(4);
      expect(tokens[0].text).toBe('pick up');
      expect(tokens[0].type).toBe(TokenType.WORD);
      expect(tokens[0].normalized).toBe('pick up');
      expect(tokens[0].isCompoundVerb).toBe(true);
      
      // Check that it maintains references to original parts
      expect(tokens[0].parts).toHaveLength(2);
      expect(tokens[0].parts[0].text).toBe('pick');
      expect(tokens[0].parts[1].text).toBe('up');
    });

    it('should handle disabled compound verb recognition', () => {
      const tokenizerNoCompounds = createEnglishTokenizer({
        recognizeCompoundVerbs: false
      });
      
      const input = 'pick up the red book';
      const tokens = tokenizerNoCompounds.tokenize(input);
      
      expect(tokens.length).toBe(5);
      expect(tokens[0].text).toBe('pick');
      expect(tokens[1].text).toBe('up');
    });

    it('should handle various compound verbs', () => {
      const compounds = [
        'look at', 'turn on', 'turn off', 'look under',
        'talk to', 'give to', 'sit on', 'get in'
      ];
      
      for (const compound of compounds) {
        const input = `${compound} something`;
        const tokens = tokenizer.tokenize(input);
        
        expect(tokens[0].text).toBe(compound);
        expect(tokens[0].isCompoundVerb).toBe(true);
      }
    });

    it('should allow adding custom compound verbs', () => {
      tokenizer.addCompoundVerb('search through');
      
      const input = 'search through the drawer';
      const tokens = tokenizer.tokenize(input);
      
      expect(tokens[0].text).toBe('search through');
      expect(tokens[0].isCompoundVerb).toBe(true);
    });

    it('should provide a list of recognized compound verbs', () => {
      const compounds = tokenizer.getCompoundVerbs();
      expect(compounds).toContain('pick up');
      expect(compounds).toContain('look at');
      
      // Add a new one and verify it's in the list
      tokenizer.addCompoundVerb('rummage through');
      expect(tokenizer.getCompoundVerbs()).toContain('rummage through');
    });
  });

  describe('combined features', () => {
    it('should handle mixed contractions and compound verbs', () => {
      const input = "don't look at the strange device";
      const tokens = tokenizer.tokenize(input);
      
      expect(tokens.length).toBe(5);
      expect(tokens[0].text).toBe("don't");
      expect(tokens[0]).toHaveProperty('expansion');
      
      expect(tokens[1].text).toBe('look at');
      expect(tokens[1].isCompoundVerb).toBe(true);
    });

    it('should handle complex input with various token types', () => {
      const input = "Let's pick up the box, look inside it, and don't forget to turn on the light.";
      const tokens = tokenizer.tokenize(input);
      
      // Check specific tokens without assuming total length (which may vary based on implementation)
      const letToken = tokens.find(t => t.text === "Let's");
      expect(letToken).toBeDefined();
      expect(letToken).toHaveProperty('expansion', 'let us');
      
      const pickUpToken = tokens.find(t => t.text === "pick up");
      expect(pickUpToken).toBeDefined();
      expect(pickUpToken.isCompoundVerb).toBe(true);
      
      const lookInsideToken = tokens.find(t => t.text === "look inside");
      expect(lookInsideToken).toBeDefined();
      expect(lookInsideToken.isCompoundVerb).toBe(true);
      
      const dontToken = tokens.find(t => t.text === "don't");
      expect(dontToken).toBeDefined();
      expect(dontToken).toHaveProperty('expansion', 'do not');
      
      const turnOnToken = tokens.find(t => t.text === "turn on");
      expect(turnOnToken).toBeDefined();
      expect(turnOnToken.isCompoundVerb).toBe(true);
    });
  });
});